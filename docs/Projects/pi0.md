---
title: Zero-Shot Prompt to Action
description: Zero-shot prompt to action on a $160 3D printed robotic arm with œÄ‚ÇÄ.
draft: true
hide:
  - navigation
  - tags
template: comments.html
---

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="Zero-Shot Prompt to Action - Teddy Warner">
  <meta name="description" content="Zero-shot prompt to action on a $160 3D printed robotic arm with œÄ‚ÇÄ.">
  <meta name="keywords" content="Robotics, VLA, Robot, LLM, VLM">
  <meta name="author" content="Teddy Warner">
  <meta name="robots" content="index, follow">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://teddywarner.org/Projects/pi0/">
  <meta property="og:title" content="Zero-Shot Prompt to Action - Teddy Warner">
  <meta property="og:description" content="Zero-shot prompt to action on a $160 3D printed robotic arm with œÄ‚ÇÄ.">
  <meta property="og:image" content="https://teddywarner.org/assets/images/pi0/hero.png">
  <meta property="og:image:type" content="image/png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">

  <!-- Twitter -->
  <meta property="twitter:card" content="summary_large_image">
  <meta property="twitter:url" content="https://teddywarner.org/Projects/pi0/">
  <meta property="twitter:title" content="Zero-Shot Prompt to Action - Teddy Warner">
  <meta property="twitter:description" content="Zero-shot prompt to action on a $160 3D printed robotic arm with œÄ‚ÇÄ.">
  <meta property="twitter:image" content="https://teddywarner.org/assets/images/pi0/hero.png">

  <!-- Existing resource links -->
  <script src="https://kit.fontawesome.com/79ff35ecec.js" crossorigin="anonymous"></script>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,200..900;1,200..900&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,200..900;1,200..900&family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../../assets/css/projects/project.css">
  <link rel="stylesheet" href="../../assets/css/projects/vnp.css">
  <link rel="stylesheet" href="../../assets/css/header.css">

  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "Zero-Shot Prompt to Action",
    "description": "Zero-shot prompt to action on a $160 3D printed robotic arm with œÄ‚ÇÄ.",
    "image": "https://teddywarner.org/assets/images/pi0/hero.png",
    "author": {
      "@id": "https://teddywarner.org/#person"
    },
    "publisher": {
      "@id": "https://teddywarner.org/#person"
    },
    "datePublished": "2025-02-10T00:00:00Z",
    "dateModified": "2025-02-10T00:00:00Z",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://teddywarner.org/Projects/pi0/"
    }
  }
  </script>
</head>

  <nav class="main-navigation">
    <ul>
      <li><a class="home" href="https://teddywarner.com"><span class="navnum">01</span> Home</a></li>
      <li><a class="proj" href="https://teddywarner.com/proj/"><span class="navnum">02</span> Projects</a></li>
      <li><a class="writ" href="https://teddywarner.com/writ/"><span class="navnum">03</span> Writing</a></li>
    </ul>
  </nav>
  
  <div class="blur-overlay"></div>

<script src="../../assets/js/header.js"></script>
<script>
  document.addEventListener('DOMContentLoaded', function() {
    initializeHeader();
  });
</script>
  
<div class="return2feed"><a href="https://teddywarner.org/proj"><i class="fa-solid fa-arrow-left-long"></i> Project Feed</a></div>

# Zero-Shot Prompt to Action

<div style="margin-top: -0.8em;">
  <span class="abtlinks"><a href="https://x.com/WarnerTeddy"><img src="https://avatars.githubusercontent.com/u/48384497" alt="Teddy Warner's GitHub profile picture" class="profilepic"><span class="abt" id="name"> Teddy Warner</a><span class="abt" style="font-weight: 300; padding-left: 6px;"><span class="year">| Spring, 2025 </span>| <span class="readTime"><i class="far fa-clock"></i> X-X minutes</span></span></span></span>
  <span class="share" style=" color: inherit;">
  <a class="fb" title="Share on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://teddywarner.org/Projects/pi0"><i class="fa-brands fa-facebook"></i></a>
  <a class="twitter" title="Share on Twitter" href="https://twitter.com/intent/tweet?url=https://teddywarner.org/Projects/pi0&text="><i class="fa-brands fa-x-twitter"></i></a>
  <a class="pin" title="Share on Pinterest" href="https://pinterest.com/pin/create/button/?url=https://teddywarner.org/Projects/pi0&media=&description="><i class="fa-brands fa-pinterest"></i></a>
  <a class="ln" title="Share on LinkedIn" href="https://www.linkedin.com/shareArticle?mini=true&url=https://teddywarner.org/Projects/pi0"><i class="fab fa-linkedin"></i></a>
  <a class="email" title="Share via Email" href="mailto:info@example.com?&subject=&cc=&bcc=&body=https://teddywarner.org/Projects/pi0%0A"><i class="fa-solid fa-paper-plane"></i></a>
  </span>
</div>

---

*Zero-shot prompt to action on a $160 3D printed robotic arm with œÄ‚ÇÄ.*

The future will undoubtedly consist of robots that ‚Äújust work‚Äù. Where autonomous machines are informed by semantics and senses in the same way we humans are.

In his 2022 paper, *A Path Towards Autonomous Machine Intelligence* Dr. Yann LeCun proposes that for ML systems to truly mimic the human ability to act in novel situations/environments, said ML systems must master both the semantic and scientific domains.[^1] They must both understand language/society and physical/time.

While current Large Language Models (LLMs) and Vision Language Models (VLMs) very accurately grasp language and society, they fall short in the later physical domains. To establish machines that ‚Äújust work‚Äù, we must fill in this hole: we must equip ML systems with an understanding of time and space.

I work full-time on this very problem at [Intempus](https://intempus.org/), equipping agents with an understanding of time and space through the collection and analysis of physiological data. Yet that's not what this piece is about, we‚Äôll be focusing on a different approach: be focusing on generalist robotics policies.

In early February 2025, [Physical Intelligence](https://www.physicalintelligence.company/), a company developing foundational robotic control policies, open-sourced œÄ‚ÇÄ, a state-of-the-art Vision Language Action (VLA) model. œÄ‚ÇÄ excels in a number of diverse tasks (folding laundry, cleaning a table, scooping coffee beans, etc.) while remaining general enough to control a variety of robot types (single arm, dual arms, mobile robots, etc.).

<center>
  <div class="tweet-container">
    <div class="tweet-item single">
      <span class="lighttweet"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Many of you asked for code &amp; weights for œÄ‚ÇÄ, we are happy to announce that we are releasing œÄ‚ÇÄ and pre-trained checkpoints in our new openpi repository! We tested the model on a few public robots, and we include code for you to fine-tune it yourself. <a href="https://t.co/1peLDU9XJ6">pic.twitter.com/1peLDU9XJ6</a></p>&mdash; Physical Intelligence (@physical_int) <a href="https://twitter.com/physical_int/status/1886822689157079077?ref_src=twsrc%5Etfw">February 4, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></span>
      <span class="darktweet"><blockquote class="twitter-tweet" data-theme="dark"><p lang="en" dir="ltr">Many of you asked for code &amp; weights for œÄ‚ÇÄ, we are happy to announce that we are releasing œÄ‚ÇÄ and pre-trained checkpoints in our new openpi repository! We tested the model on a few public robots, and we include code for you to fine-tune it yourself. <a href="https://t.co/1peLDU9XJ6">pic.twitter.com/1peLDU9XJ6</a></p>&mdash; Physical Intelligence (@physical_int) <a href="https://twitter.com/physical_int/status/1886822689157079077?ref_src=twsrc%5Etfw">February 4, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></span>
    </div>
  </div>
  <br>
</center>

Traditional robotic control policies often rely on specialized data/programming for each task, limiting their versatility. Vision Language Action models like œÄ‚ÇÄ provide a generalist baseline for autonomous machines, allowing said robots to adapt and learn new tasks with minimal additional data.

Thus robots informed by generalist policies like œÄ‚ÇÄ can truly assist/interact in diverse environments, leveraging semantic knowledge and visual understanding from ‚Äòinternet-scale‚Äô pretraining, and in turn understanding and executing complex tasks much like humans do.

œÄ‚ÇÄ is built on ‚Äòinternet-scale‚Äô vision-language pretraining, giving the model an incredibly robust visual understanding and semantic knowledge. The model extends into the physical real by incorporating action and observation state tokens, enabling the model to output continuous motor commands and high frequencies (50 Hz). Physical intelligence notes that their model was initially trained on data from 8 distinct robots, and later expanded to 7 robotic platforms with 68 unique tasks.

[PI0 robots photo]

The open-source release of œÄ‚ÇÄ is still considered an experiment: ‚ÄúœÄ‚ÇÄ was developed for [Physical Intellegence‚Äôs] own robots ‚Ä¶ and though [they] are optimistic that researchers and practitioners will be able to run creative new experiments by adapting œÄ‚ÇÄ to their own platforms, [they] do not expect every such attempt to be successful. All this is to say: œÄ‚ÇÄ may or may not work for you, but you are welcome to try it and see‚Äù![^2]

So Physical Intelligence has gifted us with this very cool model which happens to work with many expensive robots. The folks at ü§ó [Hugging Face](https://huggingface.co/) open-sourced a PyToarch port of œÄ‚ÇÄ concurrently with Physical Intelligence‚Äôs release, and it felt only right to get œÄ‚ÇÄ up and running on a cheap and sturdy robotic arm: the $160 [LeRobot So-100](https://github.com/TheRobotStudio/SO-ARM100)!

## Setup Your Environment

[LeRebot](https://huggingface.co/lerobot) is an awesome subsidiary of Hugging Face which provides open-source models, datasets, and tools for robotics - all written with PyToarch. They recently developed a new robotic arm, the So-100 which has been taking the robotics community by storm for its low cost and ease of use. This is the arm we‚Äôll be using to run œÄ‚ÇÄ, as its accessibility means that many will be able to replicate this work.

You can build a So-100 teleoperation setup for yourself by following this [LeRobot guide](https://github.com/huggingface/lerobot/blob/main/examples/10_use_so100.md), or you can purchase a prebuilt setup from [Seeed Studio](https://www.seeedstudio.com/SO-ARM100-Low-Cost-AI-Arm-Kit.html).

[s0-100 images]

[INSERT lerobot isntall + WSL paragraph]

To provide our VLA with the vision it needs, we‚Äôll be making an upgrade to the base So-100 build with a 160¬∞ FOV gripper cam. I mocked up a quick wrist mount for my camera in Fusion 360 [link to file] for my bot but also stumbled across [@cmcgartoll‚Äôs](https://x.com/cmcgartoll) redesigned S0-100 [wrist with an integrated camera mount](https://github.com/TheRobotStudio/SO-ARM100/blob/main/Optional/Camera_Holder_Alternate_MF/README.md) - either option should work.

[gripper cam photo]

In addition to the gripper cam, a third-person angle of your environment will be needed. I used a desk-mounted tripod and my phone with the droidCam app for this.

[third person cam angle]

Think of your setup as a clock: place your So-100 at 6 o‚Äôclock and your third-person camera at 12 o‚Äôclock.

[Diagram + Photos]

You can follow this [guide to set up your cameras](https://github.com/huggingface/lerobot/blob/main/examples/7_get_started_with_real_robot.md#c-add-your-cameras-with-opencvcamera) in LeRobot - this addresses streaming your phone‚Äôs cameras as well. For my implementation, this ended up being the most time-consuming part. WSL needs to have all USB devices ported to it from Windows and certainly does not get along well without video devices, so I spent a good chunk of time wrangling with my WSL kernel. Doing this natively in Mac or Windows shouldn‚Äôt be as tricky. 

Once you‚Äôve built (or bought) your robots and set up your cameras in LeRobot, you‚Äôre ready to test with teleoperation!

[teleoperation script]

If you see both camera streams pop up and can control the follower arm with the leader arm, you‚Äôre golden!

## Running Zero-Shot VLA Inference

At this point we enter a bit of a choose-your-own journey, I‚Äôve gone ahead and collected a bunch of manual teleoperation data from my So-100 setup and used it to finetune œÄ‚ÇÄ for my environment. Keep reading this section and I‚Äôll we‚Äôll cover how to use my fine-tuned œÄ‚ÇÄ weights to run an inference with your own So-100 setup. While I‚Äôve done my best to recount how my septic environment is set up, there is no guarantee that my fine-tuned weights will work with your environment. With that in mind, if you‚Äôd like to finetune œÄ‚ÇÄ for yourself, skip to the [finetuning](#fine-tuning-0) section.

[ How to run pi0 inference]

[results + media]

## Fine-Tuning œÄ‚ÇÄ

As mentioned in the previous section, while I‚Äôve done my best to recount how to recreate my exact setup for yourself, there‚Äôs no guarantee my fine-tuned weights will work perfectly for your environment. Fortunately, you can fine-tune œÄ‚ÇÄ for yourself! To get started you‚Äôll need to collect some data.

### Capture a Dataset

To capture a dataset with LeRobot, run the following script. In this, you‚Äôll specify the task description of what task you‚Äôll be manually recording, how many times you‚Äôll be repeating that task (episodes - generally 5-15 per task), how long each episode should be (in seconds), and how long you‚Äôll have between each episode to reset the environment (in seconds).

[Inser script]
[grep explanation]

After you‚Äôve collected your last episode, your new dataset will be saved to the ‚Äú/data‚Äù directory in the root of your LeRobot folder.

I captured four example datasets for you to reference while capturing your own, each consisting of 5 episodes with episode duration varying from 25-45 seconds and reset duration varying from 10-25 seconds.

[Insert dataset demo slider - iframe hugging face viz dataset]

### Augment Data

Great! You‚Äôve now collected a bunch of manual teleoperation data from your So-100 environment. Now let's augment that data to finetune œÄ‚ÇÄ.

`lerobot/scripts/augment_dataset.py` will process the dataset in your `/data` directory, augment that data, and push if to Hugging Face. Data augmentation is pretty sweet, we‚Äôre essentially 4x‚Äôing all that teleoperation data you manually captured simply bly flipping the footage and reversing the polarity of the action and state values for the base botor on the So-100 (i.e. 42.73 becomes -42.73 and vice versa)[^3]

[insert script]

### Finetune

With this newly augmented data, we‚Äôre finally ready to move into finetuning œÄ‚ÇÄ. 

Œ†‚ÇÄ is quite a substantial model and even finetuning takes ~80GB of VRAM. I certainly don‚Äôt have that on my laptop GPU, and I‚Äôm assuming very few people do, so we‚Äôll need to outsource or compute. For this, I‚Äôve used an A6000 on [Lambda Cloud](https://lambdalabs.com/service/gpu-cloud?srsltid=AfmBOor7UHpoe5QkiX44PD9Z6PWzllXJ60JWJXsk9RlRJtbaHa852Lip), but I‚Äôve also heard great things about [SF Compute](https://sfcompute.com/). It‚Äôs a good idea to run this script when you don‚Äôt need your computer, as this can take a while. My finetuning on 100 episodes took a little over 32 hours.

If its your first time using Lambda Cloud, follow this [getting started guide](https://lambdalabs.com/blog/getting-started-with-lambda-cloud-gpu-instances?srsltid=AfmBOopQrM0a5a0TLks-9QQadFuwQPijWYMoTgQKHOhGLzVgWhck79Iw). Once you‚Äôre SSH‚Äôed into your GPU instance, you‚Äôll need to install miniconda.

[insert scripts https://lambdalabs.com/blog/setting-up-a-anaconda-environment?srsltid=AfmBOora6_1ZJEaQv1quwbtTYA0xaI8RgvU-MlTLvPb6MSZEDesJCGks]


[^1]: https://openreview.net/pdf?id=BZ5a1r-kVsf
[^2]: https://www.physicalintelligence.company/blog/openpi
[^3]: https://gist.github.com/shreyasgite/3de71719c1f03439ed7278b9ba85b14b